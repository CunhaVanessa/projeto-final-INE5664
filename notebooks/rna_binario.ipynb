{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rede Neural para Classificação Binária - Spam Detection Dataset\n",
        "\n",
        "O dataset escolhido é o \"Spam Detection dataset\", aplicado à predição de mensagens spam com base em um conjunto de características, disponível no Kaggle (https://www.kaggle.com/datasets/smayanj/spam-detection-dataset). A variável alvo é `is_spam`, que assume os valores booleanos 0 (não spam) e 1 (spam). A análise foi realizada utilizando a linguagem de programação Python com as bibliotecas Pandas, Sklearn e Tensorflow.\n",
        "\n",
        "1. Pré-processamento dos Dados\n",
        "\n",
        "O dataset contém 20.000 amostras com as seguintes colunas:\n",
        "\n",
        "*   num_links (int): número de links na mensagem\n",
        "*   num_words (int): número total de palavras\n",
        "*   has_offer (bool): presença de termos promocionais (ex: “oferta”)\n",
        "*   sender_score (float): reputação do remetente\n",
        "*   all_caps (bool): se o assunto da mensagem está em letras maiúsculas\n",
        "*   is_spam (bool): variável alvo, determina se a mensagem é spam.\n",
        "    *   A probabilidade da mensagem ser spam foi aumenta se:\n",
        "        *   A quantidade de links for maior que 2\n",
        "        *   Contém ‘oferta’ de algo\n",
        "        *   Reputação do remetente for menor que 4\n",
        "        *   Assunto estar com todas as letras em maiúsculo\n",
        "        \n",
        "Os fatores acima da variável `is_spam` foram combinados com pesos diferentes. Também foi adicionado um ruído usando aleatoriedade Gaussiana para simular incertezas do mundo real. Emails são marcados como spam se a probabilidade final passar de 0.5."
      ],
      "metadata": {
        "id": "RktPHGZIpCsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação\n",
        "1. Instalação das bibliotecas"
      ],
      "metadata": {
        "id": "kfUrWhrksmSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn tensorflow matplotlib seaborn"
      ],
      "metadata": {
        "id": "jQtDHfintbXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Funções de ativação"
      ],
      "metadata": {
        "id": "8KsDNRBmt6XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_deriv(x): return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def relu(x): return np.maximum(0, x)\n",
        "def relu_deriv(x): return (x > 0).astype(float)\n",
        "\n",
        "def tanh(x): return np.tanh(x)\n",
        "def tanh_deriv(x): return 1 - np.tanh(x)**2"
      ],
      "metadata": {
        "id": "cTvPU7Att_HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções sigmoid:\n",
        "\n",
        "* sigmoid(x): Transforma valores em um intervalo entre 0 e 1. Boa para saída binária.\n",
        "* sigmoid_deriv(x): derivada de x, é simples e usada na retropropagação.\n",
        "\n",
        "Funções relu:\n",
        "* relu(x): Retorna 0 para negativos e o valor original para positivos. Mais eficiente que sigmoid em camadas ocultas.\n",
        "* relu_deriv(x): 1 para valores maiores que 0, senão 0.\n",
        "\n",
        "Funções tanh:\n",
        "* tanh(x): Saída entre -1 e 1. melhor centragem dos dados.\n",
        "* tanh_deriv(x): derivada de x.\n"
      ],
      "metadata": {
        "id": "dh3gkLBYw2f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  3. Funções de perda"
      ],
      "metadata": {
        "id": "6IkYNkxKu8b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mse_loss(y_true, y_pred): return np.mean((y_true - y_pred)**2)\n",
        "def mse_loss_deriv(y_true, y_pred): return (y_pred - y_true)\n",
        "\n",
        "def bce_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def bce_loss_deriv(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)\n",
        "    return (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))\n"
      ],
      "metadata": {
        "id": "88HVN9CXvBuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções MSE:\n",
        "* apenas por completude e para fins comparativos\n",
        "\n",
        "Funções BCE:\n",
        "* Clipping (np.clip) é usado para evitar log(0) e erro numérico. Derivada ajustada para evitar divisão por zero e escalada com tamanho do batch (len(y_true))."
      ],
      "metadata": {
        "id": "2uBBYh-ey0Sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  4. Classe da rede neural"
      ],
      "metadata": {
        "id": "qcAAEXQGvcZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from activations import sigmoid, sigmoid_deriv, relu, relu_deriv, tanh, tanh_deriv\n",
        "from losses import bce_loss, bce_loss_deriv, mse_loss, mse_loss_deriv\n",
        "\n",
        "activation_functions = {\n",
        "    'sigmoid': (sigmoid, sigmoid_deriv),\n",
        "    'relu': (relu, relu_deriv),\n",
        "    'tanh': (tanh, tanh_deriv)\n",
        "}\n",
        "\n",
        "loss_functions = {\n",
        "    'bce': (bce_loss, bce_loss_deriv),\n",
        "    'mse': (mse_loss, mse_loss_deriv)\n",
        "}\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, activation, loss, lr):\n",
        "        self.layers = layers\n",
        "        self.lr = lr\n",
        "        self.act, self.act_deriv = activation_functions[activation]\n",
        "        self.loss, self.loss_deriv = loss_functions[loss]\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        for i in range(len(layers) - 1):\n",
        "            weight = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2 / layers[i])\n",
        "            bias = np.zeros((1, layers[i+1]))\n",
        "            self.weights.append(weight)\n",
        "            self.biases.append(bias)\n",
        "\n",
        "    def forward(self, X):\n",
        "        a = X\n",
        "        self.zs, self.activations = [], [X]\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
        "            if i == len(self.weights) - 1:\n",
        "                a = sigmoid(z)\n",
        "            else:\n",
        "                a = self.act(z)\n",
        "            self.zs.append(z)\n",
        "            self.activations.append(a)\n",
        "        return a\n",
        "\n",
        "    def backward(self, y):\n",
        "        m = len(y)\n",
        "        delta = self.activations[-1] - y\n",
        "        deltas = [delta]\n",
        "        for i in range(len(self.layers)-3, -1, -1):\n",
        "            delta = np.dot(deltas[0], self.weights[i+1].T) * self.act_deriv(self.activations[i+1])\n",
        "            deltas.insert(0, delta)\n",
        "        for i in range(len(self.weights)):\n",
        "            dw = np.dot(self.activations[i].T, deltas[i]) / m\n",
        "            db = np.sum(deltas[i], axis=0, keepdims=True) / m\n",
        "            self.weights[i] -= self.lr * dw\n",
        "            self.biases[i] -= self.lr * db\n",
        "\n",
        "\n",
        "    def train(self, X, y, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = self.loss(y, y_pred)\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.6f}\")\n",
        "            self.backward(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.forward(X) > 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "S8IkasNbveNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classe NeuralNetwork implementa uma rede neural multicamada para classificação binária, feita do zero com NumPy. O construtor `__init__` define a arquitetura com base na lista de camadas `layers`, função de ativação `activation`, função de perda `loss` e taxa de aprendizado `lr`. Os pesos são inicializados com distribuição normal escalada e os vieses como zeros.\n",
        "\n",
        "O método forward executa a propagação direta, aplicando a função de ativação em cada camada, sendo sigmoid fixo na saída. O método backward realiza a retropropagação do erro, calculando os gradientes das funções de ativação e da função de perda para atualizar os pesos com gradiente descendente.\n",
        "\n",
        "O método train executa o treinamento da rede por múltiplas épocas, imprimindo a perda a cada ciclo. Por fim, o método predict gera as previsões binárias com base na saída da rede (limiar de 0.5). A implementação permite testar diferentes ativações e perdas, mantendo o foco em problemas de classificação binária."
      ],
      "metadata": {
        "id": "nFkq4z-yzqBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  5. Execução e treinamento do modelo"
      ],
      "metadata": {
        "id": "jL4WOx21vqv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from neural_network import NeuralNetwork\n",
        "\n",
        "valid_activations = ['sigmoid', 'tanh', 'relu']\n",
        "valid_losses = ['bce', 'mse']\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Uso: python main.py <activation> <loss>\")\n",
        "        print(f\"Ativações válidas: {valid_activations}\")\n",
        "        print(f\"Loss válidas: {valid_losses}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    activation = sys.argv[1]\n",
        "    loss = sys.argv[2]\n",
        "\n",
        "    if activation not in valid_activations:\n",
        "        print(f\"Erro: ativação inválida '{activation}'. Use uma das {valid_activations}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    if loss not in valid_losses:\n",
        "        print(f\"Erro: loss inválida '{loss}'. Use uma das {valid_losses}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = pd.read_csv('spam.csv')\n",
        "    X = df[['num_links', 'num_words', 'has_offer', 'sender_score', 'all_caps']].values\n",
        "    y = df['is_spam'].astype(int).values.reshape(-1, 1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.15, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    nn = NeuralNetwork(layers=[5, 16, 8, 1], activation=activation, loss=loss, lr=0.5)\n",
        "    nn.train(X_train, y_train, epochs=100)\n",
        "\n",
        "    y_pred = nn.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\nMatriz de Confusão:\")\n",
        "    print(cm)\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ra7eUtRCvtbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código executa o treinamento e a avaliação de uma rede neural para classificação binária de spam, utilizando parâmetros passados pela linha de comando. Ele exige dois argumentos: a função de ativação (sigmoid, tanh ou relu) e a função de perda (bce ou mse). Os dados são carregados do arquivo spam.csv, normalizados com StandardScaler, e divididos em treino e teste. A rede neural é então criada com arquitetura [5, 16, 8, 1], treinada por 100 épocas com taxa de aprendizado 0.5. Após o treinamento, são exibidos a matriz de confusão e o F1 Score com base nas previsões feitas sobre os dados de teste. Isso permite testar diferentes configurações de rede diretamente via terminal."
      ],
      "metadata": {
        "id": "-AUkULvy03Yl"
      }
    }
  ]
}